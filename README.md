# streamableâ€‘mcpâ€‘client

**Realâ€‘time *streaming* of MCP tools**

Streams MCPÂ â€œnotifications/messageâ€ events straight through to your UI and into the agentâ€™s conversation history, _while the tool is still running_.

`streamableâ€‘mcpâ€‘client` glues **OpenAIÂ Agents** to any MCP server that emits
**live notifications** (SSE and StreamableÂ HTTP [openai-agents SDK support pending]).
With it you can build tools that:

* push incremental results (e.g. *â€œchunkÂ #17 of your 1â€¯GB file uploadedâ€*)
* let the assistant comment on notifications
* finish with a fully coherent, singleâ€‘run chat history without polling

Behind the scenes the library:

1. **Surfaces every `notifications/message` chunk immediately** as a normal `ResponseTextDeltaEvent`, so frontâ€‘ends (web, CLI, etc.) print progress in real time.
2. **Appends the chunk to the agentâ€™s `RunResultStreaming.new_items` right away**, ensuring the LLM can reference it.
3. **Steps the agent forward exactly once** via a tiny helper patch (`Runner.continue_run`) so the next model delta reflects the fresh tool output.

The result: a chat experience where the assistant and the tool feel like a
single, smoothly streaming conversation.

### Reference servers `streamableâ€‘mcpâ€‘server`

That sibling repo ships **two demo servers**, but **âš â€¯note**:

*OpenAIÂ AgentsÂ SDKâ€¯v0.0.14* supports **only the legacy SSE transport**.
The newer **StreamableÂ HTTP MCP** endpoint is included for **futureâ€‘proofing** and interop tests, but the Python client in this project will ignore it until the SDK adds native support.

| server file (repoâ€¯root) | spec served | default port | endpoint | start command |
|-------------------------|-------------|--------------|----------|---------------|
| **`sseServer.ts`**      | **SSE MCP**Â (legacy â€“ *supported* by openaiâ€‘agents) | **3000** | `http://localhost:3000/sse` | `npm run sse` |
| `mcpServer.ts`          | StreamableÂ HTTP MCPÂ (latest spec â€“ *not yet supported* by openaiâ€‘agents) | **3000** | `http://localhost:3000/mcp` | `npm run mcp` |

> **QuickÂ start:**
> 1. `git clone https://github.com/josephbharrison/streamableâ€‘mcpâ€‘server`
> 2. `cd streamableâ€‘mcpâ€‘server && npm ci`
> 3. `npx tsx watch sseServer.ts`Â Â Â *(launches the compatible SSE server)*
> 4. In **this** repo, set the client mode to `"sse"` (default) and run `python src/main.py`.
---

# Diagram

### Call flow

```mermaid
graph LR
    %% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. Python application â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    subgraph Python_Application
        main[main.py]
        streamAgent[StreamableAgent]
        mux["StreamableAgentStream<br/>(multiplexer)"]

        main --> streamAgent
        streamAgent --> mux
    end

    %% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. MCP client wrapper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    subgraph MCP_Client_Wrapper
        mcpWrap[MCPServerSseWithNotifications]
        mux --> mcpWrap
    end

    %% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. Remote MCP server â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    subgraph MCP_Server
        sse["SSE endpoint<br/>notifications/*"]
        mcpWrap --> sse
    end
```

1. **main.py** creates a normal OpenAI Agents Agent but wires in our custom MCPServerSseWithNotifications. It then calls StreamableAgent.run_streamed() for the *longâ€‘running* tool request "stream up to 10 numbers.".

2. StreamableAgent.run_streamed() just forwards to the SDKâ€™s Runner.run_streamed() and wraps the returned RunResultStreaming instance inside a **StreamableAgentStream** multiplexer.

3. **StreamableAgentStream** keeps **two** asyncio tasks running:
  - The agentâ€™s *own* event generator (RunResultStreaming.stream_events()),
  - The **notification stream** produced by MCPServerSseWithNotifications.stream_notifications().
  - It emits a **single, unified** async stream that merges items from both.

4. Every time an SSE **notification chunk** arrives, the multiplexer
  - Exposes it immediately as a ResponseTextDeltaEvent (so the UI can print 1 2 3â€¦ in realtime),
  - Copies the text into RunResultStreaming.new_items **right now**,
  - Uses our patched helper Runner.continue_run() to step the outer agent forward **once**.
  - That lets the LLM â€œseeâ€ the latest tool output and decide what to say next while the tool is still running.

---

### OpenAI Agent Extensions

| file | what it adds |
|------|--------------|
| **mcp_extensions/server_with_notifications.py** | Subâ€‘class of the SDKâ€™s `MCPServerSse` that<br>1. opens a **second** inâ€‘memory stream for *logging* notifications, and<br>2. exposes a single async generator `stream_notifications()` that yields both **tool** notifications (from the SSE endpoint) **and** logging notifications injected by the server. |
| **mcp_extensions/streamable_agent_stream.py** | The heart of the realtime relay.<br>â€¢ Multiplexes the agentâ€‘event task **and** the notificationâ€‘task.<br>â€¢ Converts each text chunk into the minimal set of UI events (*ItemAdded â†’ ContentPartAdded â†’ TextDelta â†’ ContentPartDone*).<br>â€¢ Appends a completed `MessageOutputItem` to `run.new_items` so the LLM can reference it.<br>â€¢ Calls **`Runner.continue_run()`** (our SDK patch) to pull exactly **one** semantic event from the stillâ€‘running agent, then yields it downstream. |
| **mcp_extensions/streamable_agent.py** | Tiny convenience wrapper: given an `Agent` and an MCP server it returns a `StreamableAgentStream` each time you need a *streamed* call. |
| **main.py** | Diagnostic demo.<br>â€¢ Shows how to spin up the SSE server.<br>â€¢ Prints both raw model deltas **and** relayâ€‘injected deltas in the console (see the two `if` branches in the loop). |

---

#### Sequence Diagram

```mermaid
sequenceDiagram
    %% concrete runtime objects
    participant Main    as main.pyÂ run()
    participant SA      as StreamableAgent
    participant SAS     as StreamableAgentStream
    participant Runner  as openaiâ€‘agentsÂ Runner
    participant MCP     as MCPServerSseWithNotifications
    participant SSE     as Remote SSEÂ server

    %% 1Â construction and first call
    Main  ->> SA      : run_streamed("streamÂ upÂ toÂ 10Â numbers")
    SA    ->> Runner  : run_streamed(agent,input)
    Runner-->> SA      : RunResultStreamingÂ base_stream
    SA    -->> Main    : StreamableAgentStreamÂ instanceÂ (SAS)
    Note over Main,SAS: Main now iterates SAS.stream_events()

    %% 2Â background tasks inside SAS
    Note over SAS,MCP: TaskÂ A base_stream.stream_events()  TaskÂ B MCP.stream_notifications()

    %% 2bÂ open SSE channel for notifications
    SAS   ->> MCP     : subscribe_notifications
    MCP   ->> SSE     : HTTPÂ GET /sse
    SSE  -->> MCP     : 200 OK (events begin)

    %% 2cÂ legacy tool call via POST
    Runner->> MCP     : tools/call "stream_numbers"
    MCP   ->> SSE     : HTTPÂ POST /sse?sessionId=<id>  JSONâ€‘RPC body
    SSE  -->> MCP     : 200 OK (operation accepted)

    %% 3AÂ normal model deltas
    SAS   ->> Runner  : TaskÂ A next base_stream event
    Runner-->> SAS     : RawResponsesStreamEvent (model delta or toolâ€‘call)
    SAS   -->> Main    : same RawResponsesStreamEvent

    %% 3BÂ notifications from SSE stream
    SSE  --) MCP      : event notifications/numberÂ 1
    MCP  --) SAS      : JSONâ€‘RPC notification numberÂ 1
    SAS  --) Main     : ResponseTextDelta 1

    %% 4Â commit chunk then advance agent one step
    SAS   ->> Runner  : continue_run(base_stream)
    Runner-->> SAS     : next model delta
    SAS  --) Main     : ResponseTextDelta from LLM

    %% â€¦ numbersÂ 2Â throughÂ 10 follow the same 3B andÂ 4 cycle â€¦

    %% 5Â server closes the stream
    SSE  --) MCP      : event notifications/stream_end
    MCP  --) SAS      : stream_end sentinel
    Runner-->> SAS     : final assistant message
    SAS   -->> Main    : final assistant message
```

---

### Key methods in `StreamableAgentStream`

| method | purpose |
|--------|---------|
| `stream_events()` | Main coroutine. Runs two tasks (`agent_task`, `notif_task`), waits on whichever completes first, and yields events. Also honors the *grace period* (`_GRACE_TICKS`) so late notifications are still processed after the agent has finished. |
| `_handle_notification()` | For **one** `notifications/*` payload:<br>1&nbsp;Â· Converts its text into delta events for the UI.<br>2&nbsp;Â· Creates a *completed* assistant `MessageOutputItem` and appends it to the inâ€‘flight runâ€™s `new_items`.<br>3&nbsp;Â· Calls `Runner.continue_run()` **once** and yields that single event (usually a model delta or the final answer). |
| `_extract_text_chunks()` | Tiny helper that supports both the assistantâ€‘style `{"content":[â€¦{"type":"text"}â€¦]}` payload **and** the flat `{"data":{"type":"text","text":"â€¦"}` shape. |

---

### Why patch OpenAI Agents SDK?

#### Runner.continue_run

- The public SDK lets you **start** a streamed run and then iterate:

```
async for evt in run.stream_events(): ...
```

- But you canâ€™t say â€œgive me just the **next** event and then pauseâ€.
- The realtime relay needs exactly that granularity: after *each* notification chunk it must
  1. Wake the agent,
  2. Wait for **one** event (usually a model delta),
  3. go back to waiting for the next notification.
- continue_run() is therefore a minimal, ~20â€‘line helper that peeks one item from the internal queue, taking care to propagate errors and to notice when the background task has already finished.


When the SDK one day exposes an official step() / poll() API the patch can be dropped.

---

### Extending / modifying

- **Richer notification payloads** (e.g. images, JSON)?

  Extend _extract_text_chunks() and the UIâ€‘event construction logic.

- **Longer grace**

  Change _GRACE_TICKS (each tick =â€¯100â€¯ms).

- **Skip immediate model reaction** (passâ€‘through only)

  Remove the call to Runner.continue_run() in _handle_notification().

- **Multiple concurrent tools**

  Instantiate one StreamableAgentStream per tool invocation; each manages

  its own multiplexing.



## Patching the *openaiâ€‘agents* SDK

This repo relies on a **oneâ€‘liner helper** (`Runner.continue_run`) that is **not yet upstreamed** to *openaiâ€‘agents*.
We ship that change as a standardÂ `gitâ€‘apply` patch.

|                  | path                                    |
| ---------------- | --------------------------------------- |
| patch file       | `patches/continue_run.patch`            |
| target file      | `<venvâ€‘siteâ€‘pkgs>/agents/runner.py`     |

### Apply the patch

```bash
# from the repository root
git apply patches/continue_run.patch
```

or, if you prefer patch:
```bash
patch -p1 < patches/continue_run.patch
```

Tipâ€‚ğŸ“¦â€‚If you vendor the SDK in ./libs/openaiâ€‘agents/, run the same command inside that folder.

### Verify

```bash
python - <<'PY'
from agents.runner import Runner
assert hasattr(Runner, "continue_run"), "patch did not apply!"
print("âœ…  continue_run helper is present")
PY
```

### Revert / reâ€‘apply after upgrades
```
git apply -R patches/continue_run.patch   # â† undo
pip install --upgrade openai-agents       # upgrade SDK
git apply patches/continue_run.patch      # â† redo
```
